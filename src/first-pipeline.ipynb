{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tim's Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Timmy PC\\anaconda3\\envs\\ki-modelle\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForQuestionAnswering, T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Text generation -> Text Input: Fragen werden daraus generiert\n",
    "\n",
    "textgen_model_name = \"google/flan-t5-large\"\n",
    "textgen_model = T5ForConditionalGeneration.from_pretrained(textgen_model_name)\n",
    "textgen_tokenizer = T5Tokenizer.from_pretrained(textgen_model_name)\n",
    "\n",
    "#context = \"There are two tenses in English – past and present. The past tense in English is used to talk about the past, about hypotheses and for politeness.\"\n",
    "context = \"Fußball-Club Bayern München e. V. (FCB, pronounced [ˈfuːsbalˌklʊp ˈbaɪɐn ˈmʏnçn̩] ⓘ), also known as FC Bayern (pronounced [ˌɛft͡seː ˈbaɪɐn] ⓘ), Bayern Munich, or simply Bayern, is a German professional sports club based in Munich, Bavaria. It is best known for its professional men's association football team, which plays in the Bundesliga, the top tier of the German football league system. Bayern is the most successful club in German football history, having won a record 33 national titles, including 11 consecutively since 2013, and 20 national cups, along with numerous European honours.  \"\n",
    "\n",
    "input_text = \"Generate a question about the following text:\" + context\n",
    "input_ids = textgen_tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = textgen_model.generate(\n",
    "    input_ids,\n",
    "    max_length=250,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=5,\n",
    ")\n",
    "#print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "generated_question = textgen_tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "qa_model_name = 'deepset/roberta-base-squad2'\n",
    "\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "\n",
    "qa_pipe = pipeline(\n",
    "  'question-answering',\n",
    "  qa_model=qa_model,\n",
    "  qa_tokenizer=qa_tokenizer\n",
    ")\n",
    "\n",
    "qa_input = {\n",
    "  'question': generated_question,\n",
    "  'context': context\n",
    "}\n",
    "\n",
    "qa_res = qa_pipe(qa_input)\n",
    "\n",
    "score = qa_res['score']\n",
    "start = qa_res['start']\n",
    "end = qa_res['end']\n",
    "qa_true_answer = qa_res['answer']\n",
    "\n",
    "#display(f'Score : {score}')\n",
    "#display(f'Start : {start}')\n",
    "#display(f'End   : {end}')\n",
    "#display(f'Answer: {qa_true_answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Timmy PC\\anaconda3\\envs\\ki-modelle\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_true_answer1 = \"Generate a wrong answer for the question\" + generated_question\n",
    "input_ids = textgen_tokenizer(input_true_answer1, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs2 = textgen_model.generate(\n",
    "    input_ids,\n",
    "    max_length=250,\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=5,\n",
    ")\n",
    "\n",
    "generated_false_answer1 = textgen_tokenizer.decode(outputs2[0])\n",
    "\n",
    "# 2. Falsche Antwort\n",
    "\n",
    "input_true_answer2 = \"Generate a false answer for the following question:\" + generated_question\n",
    "input_ids = textgen_tokenizer(input_true_answer2, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs3 = textgen_model.generate(\n",
    "    input_ids,\n",
    "    max_length=250,\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=5,\n",
    ")\n",
    "\n",
    "generated_false_answer2 = textgen_tokenizer.decode(outputs3[0])\n",
    "\n",
    "# Eine Falsche Antwort wird nur sinnvoll generiert, wenn im Prompt als Kontext die Frage genommen wird. Allein passende Falsche Antworten zu der richtigen Antwort findet das Modell nicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quastion 1:\n",
      "How many national titles has Bayern won?\n",
      "Answers:\n",
      "- 33\n",
      "- ten\n",
      "- three\n"
     ]
    }
   ],
   "source": [
    "# Clean und Output\n",
    "\n",
    "clean_generated_question = generated_question.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "clean_generated_false_answer1 = generated_false_answer1.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "clean_generated_false_answer2 = generated_false_answer2.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "\n",
    "print('Question 1:')\n",
    "print(clean_generated_question)\n",
    "print('Answers:')\n",
    "print('- ' + qa_true_answer)\n",
    "print('- ' + clean_generated_false_answer1)\n",
    "print('- ' + clean_generated_false_answer2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a81cdd8a11a8047895b216a2bd951efc53cbc3c2b861ed231b31e1c241ac9ac9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
