{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Funktionierende KI-Modelle"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hier kommen KI-Modelle rein, die bei mindestens einem Teammitglied funktionieren"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Alex Felker\\anaconda3\\envs\\ki-modelle\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "# imports\n",
                "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
                "from transformers import RobertaForQuestionAnswering, AutoTokenizer\n",
                "import torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Roberta-Base-Squad2 - Question Answering Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "\"Question: What did the badger pour into the mole's ears?\""
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "'Answer  : potion'"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# question answering - roberta-large-squad\n",
                "model_name = 'deepset/roberta-large-squad2'\n",
                "\n",
                "model = RobertaForQuestionAnswering.from_pretrained(model_name)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "context = \"\"\"The Very Grateful Mole. Once there lived a mole who knew everything about trees, and was best friendswith a badger, who described everything about them, as the mole could not see. One day, the mole thought, \"hold on, if badger can see, then why can't I?\" He desperately wanted to see. So he went round to badger's house and said,\"please could you help me? because I am tired of this darkness.\" The badger replied,\" I can help you. I happen to know a very special secret potion.\" So the next day, the very kind badger went out to find the ingredients to the special potion. When he returned, he had in his paws: bark from a pine tree, sunflowerseeds from the meadow, sap from a young oak, water from the stream and gold leaves from the magic tree in the heart of the woods. He placed them all into acarved wooden bowl and hung it above his fire too cook, stirring it gently every few minutes. When it was cooled, he poured it into the mole's ears, saying, \"sleep, so that you will wake up and be able to see.\" The next day, Mole woke up, opened his eyes and thought he was in a magical dream because he had never seen anything except darkness. Until badger came round and told him, \"you are not in a dream, you can see!\" Mole said, \"this is fabulous! I can finally see what you actually look like!\" Badger helped him by describing what everything was, so that mole would understand what his eyes were showing him. Mole said to badger, \"I want to go and see the tree that helped me to see.\" So they strolled through the calm woods until they found the tree with gold leaves. Mole ran up to it and hugged it, because he was overjoyed to meet this little tree that had made a big difference to his life. But, looking up, he saw that the surrounding large oaks needed pruning because they were blocking the sunlight from the tree, causing the leaves to shrivel up. \"We'd better call for owl to help us\". Owl swooped over and asked what he could do for them. Badger explained, \"these oaks need your sharp beak to cut back their branches so that the sunlight reaches our gold leaf tree.\" Owl agreed to help, and flew up immediately to use his beak to snip off the longest branches. Sunshine poured down onto the little tree and the leaves suddenly began to glow in response. From then on, badger and mole visited the tree everyday to make sure it had enough water and light. And all was peaceful in the wonderful woods.\"\"\"\n",
                "\n",
                "questions = [\n",
                "    # 'Why did the large oaks needed pruning?',\n",
                "    # 'Why did the Mole think he was in a magical dream?',\n",
                "    # 'What ingridients did the badger get from the magic tree for the special secret potion?',\n",
                "    \"What did the badger pour into the mole's ears?\"\n",
                "]\n",
                "\n",
                "QA_results = []\n",
                "\n",
                "for question in questions:\n",
                "    inputs = tokenizer(\n",
                "        question,\n",
                "        context,\n",
                "        return_tensors='pt',\n",
                "        max_length=512,\n",
                "        truncation=True\n",
                "    )\n",
                "\n",
                "    with torch.no_grad():\n",
                "        # outputs = model(**inputs)\n",
                "        outputs = model(**inputs, return_dict=True)\n",
                "\n",
                "    # Find start and end indices with highest logits\n",
                "    start_idx = torch.argmax(outputs['start_logits']).item()\n",
                "    end_idx = torch.argmax(outputs['end_logits']).item() + 1\n",
                "\n",
                "    answer = tokenizer.decode(\n",
                "        inputs['input_ids'][0][start_idx:end_idx], skip_special_tokens=True)\n",
                "\n",
                "    QA_results.append({\n",
                "        'question': question,\n",
                "        'answer': answer\n",
                "    })\n",
                "\n",
                "for result in QA_results:\n",
                "    display(f'Question: {result[\"question\"]}')\n",
                "    display(f'Answer  :{result[\"answer\"]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Flan-T5 - Text to text Generation Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
                        "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "What did the badger pour into the mole's ears?\n"
                    ]
                }
            ],
            "source": [
                "# model_name = \"google/flan-t5-xxl\"     # hat 11B parameters, funkt bei meinem laptop nicht - Alex\n",
                "model_name = \"google/flan-t5-large\"   # hat 783M parameters, funkt bei meinem laptop - Alex\n",
                "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
                "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
                "\n",
                "# alt + z um text zu wrapen\n",
                "input_text = \"\"\"Generate three questions about the following text: The Very Grateful Mole. Once there lived a mole who knew everything about trees, and was best friendswith a badger, who described everything about them, as the mole could not see. One day, the mole thought, \"hold on, if badger can see, then why can't I?\" He desperately wanted to see. So he went round to badger's house and said,\"pleasecould you help me? because I am tired of this darkness.\" The badger replied,\" I can help you. I happen to know a very special secret potion.\" So the next day, the very kind badger went out to find the ingredients to the special potion. When he returned, he had in his paws: bark from a pine tree, sunflowerseeds from the meadow, sap from a young oak, water from the stream and gold leaves from the magic tree in the heart of the woods. He placed them all into acarved wooden bowl and hung it above his fire too cook, stirring it gently every few minutes. When it was cooled, he poured it into the mole's ears, saying, \"sleep, so that you will wake up and be able to see.\" The next day, Mole woke up, opened his eyes and thought he was in a magical dream because he had never seen anything except darkness. Until badger came round and told him, \"you are not in a dream, you can see!\" Mole said, \"this is fabulous! I can finally see what you actually look like!\" Badger helped him by describing what everything was, so that mole would understand what his eyes were showing him. Mole said to badger, \"I want to go and see the tree that helped me to see.\" So they strolled through the calm woods until they found the tree with gold leaves. Mole ran up to it and hugged it, because he was overjoyed to meet this little tree that had made a big difference to his life. But, looking up, he saw that the surrounding large oaks needed pruning because they were blocking the sunlight from the tree, causing the leaves to shrivel up. \"We'd better call for owl to help us\". Owl swooped over and asked what he could do for them. Badger explained, \"these oaks need your sharp beak to cut back their branches so that the sunlight reaches our gold leaf tree.\" Owl agreed to help, and flew up immediately to use his beak to snip off the longest branches. Sunshine poured down onto the little tree and the leaves suddenly began to glow in response. From then on, badger and mole visited the tree everyday to make sure it had enough water and light. And all was peaceful in the wonderful woods.\"\"\"\n",
                "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
                "\n",
                "outputs = model.generate(\n",
                "    input_ids,\n",
                "    max_length=512,\n",
                "    num_beams=4,\n",
                "    early_stopping=True,\n",
                "    no_repeat_ngram_size=5,\n",
                ")\n",
                "\n",
                "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
                "print(decoded_output)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.13",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        },
        "vscode": {
            "interpreter": {
                "hash": "a81cdd8a11a8047895b216a2bd951efc53cbc3c2b861ed231b31e1c241ac9ac9"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
