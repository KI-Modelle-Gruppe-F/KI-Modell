{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetunig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Define your custom dataset class\n",
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, texts, questions):\n",
    "        self.texts = texts\n",
    "        self.questions = questions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.texts[idx], 'questions': self.questions[idx]}\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare your dataset (replace this with your own data loading logic)\n",
    "texts = [...]  # List of input texts\n",
    "questions = [...]  # List of corresponding questions\n",
    "dataset = QuestionDataset(texts, questions)\n",
    "\n",
    "# Tokenize and encode the dataset\n",
    "def encode_batch(batch):\n",
    "    inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    targets = tokenizer(batch['questions'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    return inputs, targets\n",
    "\n",
    "# DataLoader for the dataset\n",
    "train_loader = DataLoader(dataset, batch_size=4, collate_fn=encode_batch, shuffle=True)\n",
    "\n",
    "# Set up training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[0]\n",
    "        targets = batch[1]\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        targets = {k: v.to(device) for k, v in targets.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, labels=targets['input_ids'])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print training loss for each epoch\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_flan_t5_large\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_flan_t5_large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"derek-thomas/squad-v1.1-t5-question-generation\")\n",
    "\n",
    "# Extract relevant data from the dataset\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Define your custom dataset class\n",
    "class QuestionDataset:\n",
    "    def __init__(self, data):\n",
    "        self.texts = data['context']\n",
    "        self.questions = data['questions']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.texts[idx], 'questions': self.questions[idx]}\n",
    "\n",
    "# Prepare your dataset\n",
    "train_dataset = QuestionDataset(train_data)\n",
    "\n",
    "# Tokenize and encode the dataset\n",
    "def encode_batch(batch):\n",
    "    inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    targets = tokenizer(batch['questions'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    return inputs, targets\n",
    "\n",
    "# DataLoader for the dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, collate_fn=encode_batch, shuffle=True)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./question_generation\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_loader,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_flan_t5_large\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_flan_t5_large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"derek-thomas/squad-v1.1-t5-question-generation\")\n",
    "\n",
    "# Extract relevant data from the dataset\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Tokenize and encode the dataset\n",
    "def encode_batch(batch):\n",
    "    inputs = tokenizer(batch['context'], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    targets = tokenizer(batch['questions'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    # Include additional keys\n",
    "    inputs = {key: value.squeeze() for key, value in inputs.items()}\n",
    "    targets = {key: value.squeeze() for key, value in targets.items()}\n",
    "\n",
    "    # Print batch size and content\n",
    "    print(\"Batch size:\", len(batch))\n",
    "    print(\"Input IDs size:\", inputs['input_ids'].size())\n",
    "    print(\"Target IDs size:\", targets['input_ids'].size())\n",
    "\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'decoder_input_ids': targets['input_ids'],\n",
    "        'decoder_attention_mask': targets['attention_mask'],\n",
    "        'labels': targets['input_ids'],\n",
    "    }\n",
    "\n",
    "# DataLoader for the dataset\n",
    "train_loader = DataLoader(train_data, batch_size=4, collate_fn=encode_batch, shuffle=True)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./question_generation\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define Trainer with DataCollator\n",
    "data_collator = lambda batch: encode_batch(batch)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  # Pass the data collator\n",
    "    train_dataset=train_data,  # Pass the dataset directly\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_flan_t5_large_squad\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_flan_t5_large_squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"derek-thomas/squad-v1.1-t5-question-generation\")\n",
    "\n",
    "# Extract relevant data from the dataset\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./question_generation\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Tokenize and encode the dataset\n",
    "train_data_encoded = train_data.map(lambda x: tokenizer(x['context'], x['questions'], padding='max_length', truncation=True, max_length=512), batched=True)\n",
    "\n",
    "# Define Trainer with DataCollator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data_encoded,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_flan_t5_large_squad\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_flan_t5_large_squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2.3 - working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex Felker\\anaconda3\\envs\\ki-modelle\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Alex Felker\\anaconda3\\envs\\ki-modelle\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 60.88 examples/s]\n",
      "100%|██████████| 3/3 [03:00<00:00, 60.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 180.1864, 'train_samples_per_second': 0.05, 'train_steps_per_second': 0.017, 'train_loss': 11.234689076741537, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "def encode_batch(batch):\n",
    "    inputs = tokenizer(batch['context'], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    targets = tokenizer(batch['questions'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    # Include additional keys\n",
    "    inputs = {key: value.squeeze() for key, value in inputs.items()}\n",
    "    targets = {key: value.squeeze() for key, value in targets.items()}\n",
    "\n",
    "    # Add decoder_input_ids key\n",
    "    targets['decoder_input_ids'] = targets['input_ids'].clone()\n",
    "\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'decoder_input_ids': targets['decoder_input_ids'],\n",
    "        'decoder_attention_mask': targets['attention_mask'],\n",
    "        'labels': targets['input_ids'],\n",
    "    }\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"derek-thomas/squad-v1.1-t5-question-generation\", split='train[:3]')\n",
    "\n",
    "# Extract relevant data from the dataset\n",
    "# train_data = dataset['train'] # the entire dataset is too large to run for my computer\n",
    "train_data = dataset\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./question_generation\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Tokenize and encode the dataset using encode_batch function\n",
    "train_data_encoded = train_data.map(encode_batch, batched=True)\n",
    "\n",
    "# Define Trainer with DataCollator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data_encoded,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_flan_t5_large_squad\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_flan_t5_large_squad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki-modelle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
